\documentclass{amsart}
%\VignetteEngine{knitr::knitr}
%\VignetteIndexEntry{Overview of durmod}
%\VignetteKeyword{Mixed proportional hazard}
%\VignetteKeyword{Competing risk}

\usepackage[utf8]{inputenc}

\newcommand{\strong}[1]{{\normalfont\fontseries{b}\selectfont #1}}
\let\pkg=\strong
\newcommand\code{\bgroup\@codex}
\def\@codex#1{{\normalfont\ttfamily\hyphenchar\font=-1 #1}\egroup}

\newcommand{\bma}{\begin{bmatrix}}
\newcommand{\ema}{\end{bmatrix}}



\title{Overview of \pkg{durmod}}

\author{Simen Gaure}
\address{Ragnar Frisch Centre for Economic Research, Oslo, Norway}
\date{June 26, 2019}

\begin{document}
\begin{abstract}
This is a walkthrough of an estimation of a generated dataset with the \pkg{durmod} package.
Also, various tunable parameters and details are provided.
\end{abstract}
\maketitle
\section{A dataset}
The \pkg{durmod} package fits a mixed proportional hazard model with competing risks
to duration data.  The model is the one from \cite{GRK07}.

Let's have a look at a generated dataset which simulates an unemployment register with
two competing risks. It was generated by \code{durdata <- datagen(5000,300)}.
<<data>>=
library(durmod)
data(durdata)
head(durdata,15)
@ 

There is an \code{id} which identifies an individual. 
The individuals have been through a process. At the outset they are
all unemployed, this is recorded by the factor \code{state}. As unemployed
they face two hazards, i.e. probabilites per time unit. 
Either they can get a job, or they can enter a labour market programme, like
a subsidized wage job or similar.

These transitions are recorded in the \code{d} factor. In our simulation,
individuals who transition to \code{"job"}, exit the dataset. If a transition
to labour market programme occurs, the state variable changes to \code{"onprogram"},
and the dummy \code{alpha} changes to 1.
It is also possible to do a \code{"none"} transition, this is typically necessary
if a covariate changes, since the model has piecewise constant explanatory covariates.
Also, when on a programme, one of the hazards disappear, it is no longer possible to
make a transition to a programme, we're already on it.

Each row of the dataset has a \code{duration}, this is the time until the transition
marked in \code{d} occurs.

In our dataset, we we have an observation period of 300, and the individuals enter
the dataset at a random time in this period. When they reach the end of the observation
period they are no longer observed. That means
that some individuals do not exit the dataset by doing a transition, but with a \code{d=="none"}.

\section{The mixed proportional hazard competing risk model}
There are two covariates, \code{x1} and \code{x2}. These are assumed to influence
the two hazards.  We also assume the \code{alpha} enters the hazard.

We model the baseline hazard for transition to job as,
\begin{equation}
  h^j(\mu^j) = \exp(x_1\beta_1^j + x_2\beta_2^j + \alpha\beta_3^j + \mu^j)
\end{equation}

The hazard for transition to programme is,
\begin{equation}
  h^p(\mu^p) = \exp(x_1\beta_1^p + x_2\beta_2^p + \mu^p)
\end{equation}

Here we have included an ``intercept'', a \(\mu\), it could equally well have been
written as a multiplicative factor \(\exp(\mu)\) instead. This
\(\exp(\mu)\)-term is the ``proportional hazard''. 

The likelihood for a single observation \(k\) consists of two parts.  Let 
\(H(\mu) = h^j(\mu^j) + h^p(\mu^p)\) be the sum of the hazards, where
\(\mu\) is the vector \((\mu^j, \mu^p)\).

For an observation \(k\) there is 
a survival probability/density up until the transition:
\begin{equation}\label{survprob}
s_k(\mu) = \exp(-t_k H(\mu)), 
\end{equation}
where \(t_k\) is the duration of the period.

If there is a transition, \(s(\mu)\) is multiplied by the transition hazard, \(h^d(\mu)\), where
\(d\) is either \(p\) or \(j\). If there is no transition, \(h^d(\mu)\) is taken to be 1.
Taken together, all the observations for an individual \(i\) yields an individual likelihood.
We call it \(\ell_i(\mu)\).
\begin{equation}
  \ell_i(\mu) = \prod_{k\in K_i} h^{d_k}(\mu) s_k(\mu),
\end{equation}
where \(K_i\) is the set of observations for individual \(i\).

However, there is also a mixture part, designed to account for unobserved individual
heterogeneity. The \(\mu\)-vector is stochastic with a
discrete distribution.  That is, there is an \(n\), a set of probabilites \(p_j\), 
and vectors \(\mu_j\), for \(j=1..n\). Of course, we have \(\sum_{j=1}^n p_j = 1\).

The mixture likelihood for an individual \(i\) is \(L_i = \sum_j p_j \ell_i(\mu_j)\).

The log-likelihood for the dataset is thus, \(L=\sum_i \log(L_i)\).

The \(L\) must be maximized with respect to the five \(\beta\)s, the \(n\), the probabilites \(p_j\), 
and the vectors \(\mu_j\) for \(j=1..n\).

\section{Estimation}
The estimation proceeds as follows. We start with \(n=1\), estimate
the \(\beta\)s and the two \(\mu\)s.  Then we increase \(n\) to 2, let
\(p_2\) be a small probability, and find a vector \(\mu_2\) which
increases the likelihood.  This is used as starting point for a new likelihood maximization.
Then \(n\) is increased to \(3\), and we proceed in this fashion, adding masspoints to the
distribution until we are no longer able to increase the likelihood.

In \pkg{durmod} we use the \code{mphcrm} function for this purpose. Here is an example.
First we create a ``riskset'', a specification of which hazards are experienced in various states:
<<>>=
risksets <- list(unemp=c('job','program'), onprogram='job')
@ 

Note that the names of the list \code{risksets} are
the same as the levels in the factor \code{state}. And that the entries in the list
are levels of the factor \code{d}, i.e. possible transitions.

Then we create a set of control parameters. Since this vignette is to
be created by the busy CRAN repository, we limit ourselves to 4
iterations, i.e. no more than 4 masspoints in the distribution. For
the same reason we also limit to 1 cpu, or threads, in the
computation. The default is to use all the available cpus/cores.

<<>>=
ctrl <- mphcrm.control(iters=4, threads=1)
@ 

Then we are ready to estimate. There are a couple of special terms in the formula we use:
<<>>=
set.seed(42) # for reproducibility
opt <- mphcrm(d ~ x1 + x2 + 
                C(job, alpha) + ID(id) + D(duration) + S(state), 
              data=durdata, risksets=risksets, control=ctrl)
@ 

The left hand side of the formula, \code{d}, is the outcome, the transition that is taken.
The \code{C(job, alpha)} term is a list of conditional covariates, the \code{alpha}
should only explain the \code{"job"} transition. The \code{ID(id)} specifies
that the covariate \code{id} identifies individuals. The \code{D(duration)} specifies
that the covariate \code{duration} contains the durations of the observations. Finally,
the \code{S(state)} term specifies that the covariate \code{state} is a factor which
indexes into the \code{risksets} argument. 

\code{mphcrm} writes diagnostic output, one line per iteration. It contains
potentially useful information. There is a time stamp, the iteration number, the number
of masspoints, the resulting log likelihood, the 2-norm of the gradient, the smallest
probability in the masspoint distribution, the reciprocal condition number of the Fisher matrix,
the entropy of the masspoint distribution, and the time used in the iteration.

\code{mphcrm} returns a list with one entry for each iteration, it has a special
print method which sums up the estimation in reverse order:
<<>>=
print(opt)
@ 

Unless something has gone wrong, you will normally be interested in the first entry,
the one with the largest likelihood. We can look at a summary:
<<>>=
best <- opt[[1]]
summary(best)
@ 

It has three entries, \code{"loglik"}, which is simply the log likelihood,
\code{"coefs"} which is the values and standard errors of the estimated
coefficients. And \code{"moments"}, which is the first and second moments of
the proportional hazard distribution.

We can see how the \code{alpha} changes with more points:
<<>>=
t(sapply(opt, function(o) summary(o)$coefs["job.alpha",]))
@ 

Here is a pre-made fit:
<<>>=
summary(fit[[1]])
@ 
For an explanation of the warning about ``NaNs produced'', see the
\code{vignette("gorydetails")}.

The full estimation can be rerun with the commands:
<<eval=FALSE>>=
library(durmod)
data(durdata)
newfit <- eval(attr(fit,'call'))
@ 

There are also some functions for extracting the proportional hazard distribution:
<<>>=
round(mphdist(fit[[1]]),6)
# and the moments,
mphmoments(fit[[1]])
# and covariance matrix
mphcov(fit[[1]])
@ 
The true values used to generate the dataset was 
\code{job.x1=1}, \code{job.x2=-1}, \code{job.alpha=0.2}, \code{program.x1=1}, and
\code{program.x2=0.5}. 
The unobserved distribution, the proportional hazards,
had means: \code{job: 0.135}, \code{program: 0.0826}. Their variances were 0.0273 and 0.0113.
Their covariance was 0.00656. We see that our estimates are in the vicinity, except
for the covariances which require more data to get precise.


\section{More options}
\subsection{Interval timing}
The example above had exactly recorded time. For some applications we do have that, while
in other applications we only have a time interval when the transition is known to have taken
place. The data above is actually a prime example, perhaps we only have labour data on a monthly basis.
When a transition takes place, it is only registered at the end of the month, and there is no
record of the day. In this case, the \code{duration} would be 1 for every observation, and one
should use the \code{timing="interval"} argument in \code{mphcrm}.  The observation likelihood
is replaced by,
\begin{equation}
h^{d_k}(\mu) \exp(-t_k H(\mu)) \frac{1-\exp(-t_k H(\mu))}{H(\mu)}.
\end{equation}
It is the fractional part which distinguishes it from the exact model.

If the hazards are small and we use unit intervals, 
the difference between the interval and exact model is quite small, so
one may opt for using the exact model instead.

\subsection{No timing}
In some applications there isn't any time. A transition occurs, or not. In this case
the specification \code{timing="none"} can be used. It will use a logit model for the 
transition probabilities.

\subsection{Factors}
\code{mphcrm} treats factors specially. There is, I think, nothing special to see, but
internally \code{mphcrm} does not create a large model matrix filled with dummy variables. This means
that factors with many levels is quite fast to estimate.

\section{Control parameters}
There are many control parameters. Here are some you may want to tinker with.
\begin{itemize}
\item{\code{threads.}} An integer. The number of parallel threads used
  by \code{mphcrm}. The default is taken from
  \code{getOption("durmod.threads")}, which is initialized from the
  environment variable \code{DURMOD\_THREADS}, \code{OMP\_NUM\_THREADS},
  \code{OMP\_THREAD\_LIMIT}, \code{NUMBER\_OF\_PROCESSORS}, or else from
  \code{parallel::detectCores()}. 
  
  It is not always true that the estimation
  runs twice as fast on twice as many cpus. This depends on the cpu- and
  memory architecture of your computer, as well as on the implementation of OpenMP in
  the compiler used to compile the C++ parts of \pkg{durmod}. Besides, not all parts
  of \pkg{durmod} run in parallel, so by Amdahl's law you may not expect linear speedup when the number
  of cpus tends to infinity.

  Also, if you intend to use your computer for something else while \code{mphcrm} runs, you
  should not give it all your cpus, but save one or two for your other work. If one of the 16 threads
  in \code{mphcrm} shares a cpu with your mail program trying to sort your inbox, the speed may be
  halved.

  For creation of the Fisher matrix, \code{mphcrm} calls into BLAS from a single thread. For
  large datasets with many coefficients to estimate, there can be some benefit from linking R
  with a highly optimized and parallel BLAS, like mkl from Intel. See the R documentation for how
  to do this.
\item{\code{iters.}} An integer. The number of iterations to perform. The estimation may stop earlier, if
  neither the log likelihood \emph{nor} the entropy improves. The default is 15.
\item{\code{ll.improve.}} A numeric. The amount the log-likelihood must increase with to be considered an
  improvement. The default is 0.001.
\item{\code{e.improve}} A numeric. The amount the entropy of the hazard distribution must increase with
  to be considered an improvement. The default is 0.001.
\item{\code{callback.}} A function. If the one-line diagnostic from \code{mphcrm} is insufficient,
  it is possible to write your own. It will replace the default callback (which you can call from your function).
  In this way you can e.g. get diagnostics on particular coefficients, save intermediate results
  to file, or other partakings. See \code{mphcrm.callback}.
\item{\code{jobname.}} A character string. The initial portion of the one-line diagnostic. Useful if you
  e.g. use \code{parallel::mclapply} to run several estimations in parallel. They can have individual names so
  you can see the progress. The default is \code{"mphcrm"}.
\item{\code{method.}} A character string. Either \code{"BFGS"} (the default), or \code{"L-BFGS-B"}. The latter
  saves some memory, if that is a problem with estimations with a very large number of coefficients.
\item{\code{trap.interrupt.}} A logical. If you decide to interrupt an interactive estimation before
  it has terminated, either because you don't want to wait, or because it seems to have run astray,
  the default behaviour for \code{mphcrm} is to catch the interrupt, and return gracefully with the
  result of the estimation so far. This behaviour can be switched off with \code{trap.interrupt=FALSE}.
\item{\code{cluster.}} Cluster specification from package \pkg{parallel} or \pkg{snow}.
  In addition to utilizing all the cores/cpus on a computer, \code{mphcrm} may also
  spread across several computers. It supports running on a cluster 
  from package \code{parallel} or \code{snow}. The dataset will be
  split among the cluster nodes, with approximately equally many observations on each. The nodes will
  then do their share of the likelihood computations. If using a cluster, the \code{threads} parameter will
  be the number of cpus used on each cluster node.
  In general, when using parallelization, one should make sure that the cpus are not overbooked and that
  the nodes you are running on are approximately equally fast. 
\end{itemize}

\bibliographystyle{amsplain}
\bibliography{biblio}

\end{document}

